{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Text for Hardcode Text Analytics in Opt Out\n",
    "\n",
    "#### There are a million ways to skin a cat (poor cat), and it's quite similar when preparing text data to study it. In this notebook, we hope to show you the different steps involved to get the most information from your text possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by reading in the data. We try to be consistent at Opt Out. The data consists of comments under the column 'text' and their labels under column 'label'. Our labeling schema is 1 if it is misogynistic and 0 if not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The new Doras cute af</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@minniemonikive well</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rolou o skank</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@AOC https//t.co/lbNOwMK1p2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@tangletorn We will be killed by a snake 3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         text  label\n",
       "0                       The new Doras cute af      0\n",
       "1                        @minniemonikive well      0\n",
       "2                               Rolou o skank      1\n",
       "3                 @AOC https//t.co/lbNOwMK1p2      1\n",
       "4  @tangletorn We will be killed by a snake 3      0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from src.data.preprocess_text_helpers import (\n",
    "    contractions_unpacker,\n",
    "    punctuation_cleaner,\n",
    "    remove_stopwords,\n",
    "    tokenizer,\n",
    ")\n",
    "\n",
    "from src.data.preprocess_text_pipelines import (\n",
    "  clean,\n",
    "  tokenize,\n",
    "  normalize,\n",
    ")\n",
    "\n",
    "\n",
    "from src.data.retrieve_data_from_s3_bucket import download_dataset\n",
    "\n",
    "download_dataset(\"../data/processed/stanford.csv\")\n",
    "df = pd.read_csv(\"../data/processed/stanford.csv\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! We like consistency.\n",
    "\n",
    "\n",
    "Sadly social media data is anything but consistent. For example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@ATX_fight_club @AOC JFK was a clandestine austerity democratic. What the fk happened?!'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[13, 'text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's incredibly **messy**. Spelling mistakes, grammar failures and emojis/hashtags/urls make understanding the content of the text hard.  \n",
    "\n",
    "\n",
    "\n",
    "So how are we going to tidy this up? Can we polish our diamond in the rough to better understand the text of the tweets? \n",
    "\n",
    "**Well I'm glad you asked...** \n",
    "\n",
    "\n",
    "There are many steps you can take. There is not one right answer, it is a case by case thing. Nevertheless, Opt Out tries to make it easy for someone to get through the boring preprocessing work and into the nitty gritty text analyitics. Let's see what we can do in Opt Out\n",
    "\n",
    "\n",
    "We can either clean, tokenize or normalize the text. Let's start with tokenize. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "Tokenization is the act of breaking up a sequence of strings into pieces such as words, keywords, phrases, symbols and other elements called tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tcake/coding_projects/python/opt_out/study-online-misogyny/venv/lib/python3.7/site-packages/ekphrasis/classes/tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
      "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                         the new doras cute af\n",
       "1                          @minniemonikive well\n",
       "2                                 rolou o skank\n",
       "3            @aoc https / / t . co / lbnowmk1p2\n",
       "4    @tangletorn we will be killed by a snake 3\n",
       "Name: tokenized, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(df).head()['tokenized']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what can you see that's different? Lowercase? Is the punctuation still there? What's going on in this tokenization step? Let's take an average trolly sentence and we'll walk you through each step of our tokenization pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "troll_speak = \"RT @baum_erik: Lol I'm not surprised these 2 accounts blocked me @femfreq #FemiNazi#Gamergate &amp; @MomsAgainstWWE #ParanoidParent \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Messy, gross. Let's start by unpacking the contraction I'm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @baum_erik: Lol I am not surprised these 2 accounts blocked me @femfreq #FemiNazi#Gamergate &amp; @MomsAgainstWWE #ParanoidParent '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contractions_unpacker(troll_speak) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See `I'm is now I am`? that's called unpacking a contraction. Now we break our sentence up into tokens. Watch the final #FemiNazi#Gamergate hashtags at the end of the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"RT @baum_erik : Lol I ' m not surprised these 2 accounts blocked me @femfreq #FemiNazi #Gamergate & @MomsAgainstWWE #ParanoidParent\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(troll_speak)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok doesn't seem particularly interesting, but think about it. How would you normally split text up? We use tech that allows us to do social media tokenization, break up the text into meaningful chunks for social media data like hashtags, emojis etc.\n",
    "\n",
    "### Clean\n",
    "So now we can break the text up into pieces, but let's remove some rubbish.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             the new doras cute af\n",
       "1              @minniemonikive well\n",
       "2                       rolou skank\n",
       "3    @aoc https / / co / lbnowmk1p2\n",
       "4     @tangletorn we killed snake 3\n",
       "Name: cleaned, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean(df).head()['cleaned']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the difference? Yup, punctuation and the removal of something call stopwords. Stopwords are unimportant words, like and, with. These words are important, but not for modeling. The extra steps we take to get here are show below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"RT @baum_erik: Lol I'm not surprised these 2 accounts blocked me @femfreq #FemiNazi#Gamergate &amp; @MomsAgainstWWE #ParanoidParent \""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation_cleaner(troll_speak)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing to remove here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"RT @baum_erik: Lol I'm surprised 2 accounts blocked @femfreq #FemiNazi#Gamergate &amp; @MomsAgainstWWE #ParanoidParent \""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(troll_speak)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words like am are removed, which helps us focus on the interesting words.\n",
    "\n",
    "### Normalization\n",
    "Finally my favourite step. We don't care about all the different users, the different hashtags, quite often all we care about are the densities of these.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading english - 1grams ...\n",
      "You can't omit/backoff and unpack hashtags!\n",
      " unpack_hashtags will be set to False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tcake/coding_projects/python/opt_out/study-online-misogyny/venv/lib/python3.7/site-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0               the new doras cute af\n",
       "1                         <user> well\n",
       "2                         rolou skank\n",
       "3    <user> https / / co / lbnowmk1p2\n",
       "4            <user> we killed snake 3\n",
       "Name: normalized, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize(df).head()['normalized'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this one is a little more involved, but it produces a little cooler results. I love this method. What is does it normalize the text, we don't really care about the different urls, hashtags etc in the text, we care about the number of them per tweet. This method allows us to not care, but care all at the same time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
